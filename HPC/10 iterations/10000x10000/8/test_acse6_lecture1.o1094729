# mpiexec: MPI is /apps/mpi/intel/2019.6.166
# mpiexec: RDMA network found -- using rdma+shared memory (FI_VERBS_IFACE=ib0)
# mpiexec: full path to program is /rdsgpfs/general/user/va719/home/game_of_life
# mpiexec: program arguments are: 
# mpiexec: 24 ranks allocated via PBS select, but -n argument reduces that to 8
# mpiexec: 1 OpenMP threads / rank allocated by PBS select
# mpiexec: Job has shared use of the allocated nodes. Disabling process-pinning
# mpiexec: machinefile configured as:
r4i0n0.cx2.hpc.ic.ac.uk:24
#
# mpiexec: Checking all nodes are ONLINE:
# mpiexec: All nodes appear ONLINE
# mpiexec: launch started at Wed Feb 19 09:35:50 GMT 2020
# mpiexec: launching program...
# /rdsgpfs/general/user/va719/home/game_of_life 
(r4i0n0.cx2.hpc.ic.ac.uk:0,1,2,3,4,5,6,7)

[0] MPI startup(): I_MPI_FALLBACK environment variable is not supported.
[0] MPI startup(): Similar variables:
	 I_MPI_MALLOC
[0] MPI startup(): To check the list of supported variables, use the impi_info utility or refer to https://software.intel.com/en-us/mpi-library/documentation/get-started.
----------------------------
Divide 8 into 2 by 4 grid
----------------------------
 iteration: 0 core: 3
 iteration: 0 core: 7
 iteration:  iteration: 0 core: 5 iteration: 0 core: 6
0 core: 4

 iteration: 0 core: 0
 iteration: 0 core: 2
 iteration: 0 core: 1
 iteration: 1 core: 6
 iteration: 1 core: 2
 iteration: 1 core: 3
 iteration: 1 core: 4
 iteration: 1 core: 5
 iteration: 1 core: 7
 iteration: 1 core: 1
 iteration: 1 core: 0
 iteration: 2 core: 2
 iteration: 2 core: 6
 iteration: 2 core: 4
 iteration: 2 core: 5
 iteration: 2 core: 7
 iteration: 2 core: 3
 iteration: 2 core: 1
 iteration: 2 core: 0
 iteration: 3 core: 2
 iteration: 3 core: 6
 iteration: 3 core: 4
 iteration: 3 core: 5
 iteration: 3 core: 7
 iteration: 3 core: 3
 iteration: 3 core: 1
 iteration: 3 core: 0
 iteration: 4 core: 2
 iteration: 4 core: 6
 iteration: 4 core: 4
 iteration: 4 core: 5
 iteration: 4 core: 7
 iteration: 4 core: 3
 iteration: 4 core: 1
 iteration: 4 core: 0
 iteration: 5 core: 2
 iteration: 5 core: 6
 iteration: 5 core: 4
 iteration: 5 core: 5
 iteration: 5 core: 7
 iteration: 5 core: 3
 iteration: 5 core: 1
 iteration: 5 core: 0
 iteration: 6 core: 2
 iteration: 6 core: 6
 iteration: 6 core: 7
 iteration: 6 core: 4
 iteration: 6 core: 5
 iteration: 6 core: 3
 iteration: 6 core: 1
 iteration: 6 core: 0
 iteration: 7 core: 2
 iteration: 7 core: 6
 iteration: 7 core: 4
 iteration: 7 core: 5
 iteration: 7 core: 7
 iteration: 7 core: 3
 iteration: 7 core: 1
 iteration: 7 core: 0
 iteration: 8 core: 2
 iteration: 8 core: 6
 iteration: 8 core: 4
 iteration: 8 core: 5
 iteration: 8 core: 7
 iteration: 8 core: 3
 iteration: 8 core: 1
 iteration: 8 core: 0
 iteration: 9 core: 2
 iteration: 9 core: 6
 iteration: 9 core: 4
 iteration: 9 core: 5
 iteration: 9 core: 7
 iteration: 9 core: 3
 iteration: 9 core: 1
 iteration: 9 core: 0
No of Processors --->  8
Dimensions: RowxColoumn --->  10000 x 10000
Time Taken --->9.0272 s
# mpiexec: finished at Wed Feb 19 09:35:59 GMT 2020

============================================

        Job resource usage summary 

                 Memory (GB)    NCPUs
 Requested  :         2            24
 Used       :         0 (peak)   6.40 (ave)

============================================
